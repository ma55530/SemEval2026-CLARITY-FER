{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing the necessary libraries**"
      ],
      "metadata": {
        "id": "1pclIURvraWE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE-6wjEfrSAm"
      },
      "outputs": [],
      "source": [
        "pip -q install scikit-learn torch pandas datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset"
      ],
      "metadata": {
        "id": "IJXVboHZrVPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the data**"
      ],
      "metadata": {
        "id": "CUVP4_AJsvgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "df = load_dataset(\"ailsntua/QEvasion\")\n",
        "\n",
        "def clarity_to_label(row) :\n",
        "  mapping = {\n",
        "      \"Clear Reply\": 0,\n",
        "      \"Ambivalent\": 1,\n",
        "      \"Clear Non-Reply\": 2\n",
        "  }\n",
        "  row[\"label\"] = mapping[row[\"clarity_label\"]]\n",
        "  return row\n",
        "\n",
        "df = df.map(clarity_to_label)\n",
        "y_test = df[\"test\"][\"label\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "NRTDHxmFs6xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of Clear Replies: {df[\"train\"][\"label\"].count(0)}\")\n",
        "print(f\"Number of Ambivalent: {df[\"train\"][\"label\"].count(1)}\")\n",
        "print(f\"Number of Clear Non-Replies: {df[\"train\"][\"label\"].count(2)}\")"
      ],
      "metadata": {
        "id": "Q1gyTXNiv-M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the evaluation**"
      ],
      "metadata": {
        "id": "nN0D-pt9yQgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def evaluate(y_test, y_test_pred):\n",
        "\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_test, y_test_pred, average=\"micro\"\n",
        "    )\n",
        "    print(f\"Precision: {precision:6.2f}\")\n",
        "    print(f\"   Recall: {recall:6.2f}\")\n",
        "    print(f\"       F1: {f1:6.2f}\")\n",
        "\n",
        "    ConfusionMatrixDisplay.from_predictions(y_test, y_test_pred)\n",
        "    plt.show()\n",
        "    return precision, recall, f1\n",
        "\n"
      ],
      "metadata": {
        "id": "11IjYlyFyw61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading the model and tokenizer**"
      ],
      "metadata": {
        "id": "r1hMCrvF3ZK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=3\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# we want to cut texts longer than 512 tokens to be 512 tokens\n",
        "clf = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=lambda x, **kwargs: tokenizer(\n",
        "        x,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ),\n",
        "    device=0, # use GPU\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BmP5s3bC3gq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preparing the data for training**"
      ],
      "metadata": {
        "id": "VkrH4UON_dU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization, no longer than 512 tokens\n",
        "tokenized_train = df[\"train\"].map(\n",
        "    lambda x: tokenizer(\n",
        "        \"Question: \" + x[\"interview_question\"] + \" Answer: \" + x[\"interview_answer\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        truncation=True\n",
        "    ),\n",
        "    batched=False\n",
        ")\n",
        "\n",
        "tokenized_test = df[\"test\"].map(\n",
        "    lambda x: tokenizer(\n",
        "        \"Question: \" + x[\"interview_question\"] + \" Answer: \" + x[\"interview_answer\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        truncation=True\n",
        "    ),\n",
        "    batched=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "sLqGCRXu_iNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining the evaluation function and training**"
      ],
      "metadata": {
        "id": "VbbQ12O2qIA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    import numpy as np\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    p, r, f1 = evaluate(y_test, predictions)\n",
        "    return {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
        "\n",
        "\n",
        "\n",
        "# train model\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        "    args=TrainingArguments(\n",
        "        num_train_epochs=3,\n",
        "        eval_strategy=\"epoch\",\n",
        "        eval_steps=1,\n",
        "        output_dir=\"output\",\n",
        "        seed=42,\n",
        "        fp16=True,\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "EP3XT3BoqONl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}