{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7cMl/I0+1RbMm2xx2hqh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma55530/SemEval2026-CLARITY-FER/blob/main/multi_task/multi_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, accuracy_score\n",
        "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "aGEF78tLg2gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "jnZ7s6cdxDwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHPs5h9ZgCLF"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiTaskModel(nn.Module):\n",
        "    def __init__(self, transformer_model, num_clarity=3, num_evasion=9):\n",
        "        super(MultiTaskModel, self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "\n",
        "        # Classification and Sentiment heads\n",
        "        self.clarity_head = nn.Linear(self.transformer.config.hidden_size, num_clarity)\n",
        "        self.evasion_head = nn.Linear(self.transformer.config.hidden_size, num_evasion)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels_clarity=None, labels_evasion=None):\n",
        "        # Pass through transformer\n",
        "        transformer_output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_embedding = transformer_output.last_hidden_state[:, 0, :]  # CLS token\n",
        "\n",
        "        # Outputs for each task\n",
        "        clarity_logits = self.clarity_head(cls_embedding)\n",
        "        evasion_logits = self.evasion_head(cls_embedding)\n",
        "\n",
        "        # Calculate losses if labels are provided\n",
        "        loss = None\n",
        "        if labels_clarity is not None and labels_evasion is not None:\n",
        "            # Loss for clarity\n",
        "            loss_clarity = F.cross_entropy(clarity_logits, labels_clarity)\n",
        "\n",
        "            # Loss for evasion, handling placeholder -1 for test set\n",
        "            valid_evasion_indices = (labels_evasion != -1)\n",
        "            if valid_evasion_indices.any():\n",
        "                loss_evasion = F.cross_entropy(evasion_logits[valid_evasion_indices], labels_evasion[valid_evasion_indices])\n",
        "                # Combine losses, typically with some weighting\n",
        "                loss = loss_clarity + loss_evasion\n",
        "            else:\n",
        "                # If no valid evasion labels (e.g., during full test set evaluation if all are -1),\n",
        "                # only use clarity loss.\n",
        "                loss = loss_clarity\n",
        "\n",
        "        # The Trainer expects a specific return format.\n",
        "        # If loss is calculated, it should be the first element.\n",
        "        # Then, the model outputs for metrics.\n",
        "        if loss is not None:\n",
        "            # Return (loss, (clarity_logits, evasion_logits))\n",
        "            return (loss, (clarity_logits, evasion_logits))\n",
        "        else:\n",
        "            # During prediction/evaluation without labels, return just the logits.\n",
        "            return (clarity_logits, evasion_logits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading dataset...\")\n",
        "df = load_dataset(\"ailsntua/QEvasion\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A7HVvpugUDI",
        "outputId": "dad04ba3-a3e2-40be-ae47-1367811cdeb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "transformer_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = MultiTaskModel(transformer_model, num_clarity=3, num_evasion=9)"
      ],
      "metadata": {
        "id": "WEpFVpNThMpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"train\"].column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF0WqrFBsj5f",
        "outputId": "4127ef29-2bbe-48d0-dc4e-98ea323a8ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['title', 'date', 'president', 'url', 'question_order', 'interview_question', 'interview_answer', 'gpt3.5_summary', 'gpt3.5_prediction', 'question', 'annotator_id', 'annotator1', 'annotator2', 'annotator3', 'inaudible', 'multiple_questions', 'affirmative_questions', 'index', 'clarity_label', 'evasion_label']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clarity2id = {\n",
        "    \"Clear Reply\": 0,\n",
        "    \"Ambivalent\": 1,\n",
        "    \"Clear Non-Reply\": 2,\n",
        "}\n",
        "\n",
        "evasion2id = {\n",
        "    \"Claims ignorance\": 0,\n",
        "    \"Clarification\": 1,\n",
        "    \"Declining to answer\": 2,\n",
        "    \"Deflection\": 3,\n",
        "    \"Dodging\": 4,\n",
        "    \"Explicit\": 5,\n",
        "    \"General\": 6,\n",
        "    \"Implicit\": 7,\n",
        "    \"Partial/half-answer\": 8,\n",
        "}\n"
      ],
      "metadata": {
        "id": "Rck9SkcB0utE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_clarity(example):\n",
        "    encoded = tokenizer(\n",
        "        example[\"interview_question\"] +\n",
        "        \" | Answer: \" + example[\"interview_answer\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "    )\n",
        "    encoded[\"labels\"] = torch.tensor(clarity2id[example[\"clarity_label\"]])\n",
        "    return encoded\n",
        "\n",
        "def tokenize_evasion_train(example):\n",
        "    encoded = tokenizer(\n",
        "        example[\"interview_question\"] +\n",
        "        \" | Answer: \" + example[\"interview_answer\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "    )\n",
        "    encoded[\"labels\"] = torch.tensor(evasion2id[example[\"evasion_label\"]])\n",
        "    return encoded\n",
        "\n",
        "def tokenize_evasion_test(example):\n",
        "    # This function is not used directly for tokenization + labels anymore in the new setup\n",
        "    # but keep it for consistency if other parts might reference it.\n",
        "    encoded = tokenizer(\n",
        "        example[\"interview_question\"] + \" | Answer: \" + example[\"interview_answer\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "    )\n",
        "    return encoded\n",
        "\n",
        "def get_evasion_annotator_labels(example):\n",
        "    labels_evasion_annotators = [-1, -1, -1] # Default placeholder for 3 annotators\n",
        "    valid_count = 0\n",
        "    for ann_key in [\"annotator1\", \"annotator2\", \"annotator3\"]:\n",
        "        label_str = example.get(ann_key)\n",
        "        if label_str and label_str in evasion2id:\n",
        "            labels_evasion_annotators[valid_count] = evasion2id[label_str]\n",
        "            valid_count += 1\n",
        "    return {\"labels_evasion_annotators\": torch.tensor(labels_evasion_annotators, dtype=torch.long)}"
      ],
      "metadata": {
        "id": "YgrF1rnGjQmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokeniziraj\n",
        "clarity_tokenized_train = df[\"train\"].map(tokenize_clarity)\n",
        "clarity_tokenized_test  = df[\"test\"].map(tokenize_clarity)\n",
        "evasion_tokenized_train = df[\"train\"].map(tokenize_evasion_train)\n",
        "\n",
        "# Prepare multi-annotator labels for the test set\n",
        "evasion_annotators_test_mapped = df[\"test\"].map(get_evasion_annotator_labels, remove_columns=df[\"test\"].column_names)\n",
        "\n",
        "# 2. Rename labels -> labels_clarity / labels_evasion\n",
        "clarity_tokenized_train = clarity_tokenized_train.rename_column(\"labels\", \"labels_clarity\")\n",
        "clarity_tokenized_test  = clarity_tokenized_test.rename_column(\"labels\", \"labels_clarity\")\n",
        "evasion_tokenized_train = evasion_tokenized_train.rename_column(\"labels\", \"labels_evasion\")\n",
        "\n",
        "# 3. Pretvori evasion u int i spoji u clarity dataset\n",
        "evasion_labels_train = [int(x) for x in evasion_tokenized_train[\"labels_evasion\"]]\n",
        "train_dataset = clarity_tokenized_train.add_column(\"labels_evasion\", evasion_labels_train)\n",
        "\n",
        "# Test dataset (placeholder -1 for model's labels_evasion, and add annotator labels for compute_metrics)\n",
        "test_dataset = clarity_tokenized_test.add_column(\"labels_evasion\", [-1]*len(clarity_tokenized_test))\n",
        "test_dataset = test_dataset.add_column(\"labels_evasion_annotators\", evasion_annotators_test_mapped[\"labels_evasion_annotators\"])\n",
        "\n",
        "# 4. Tek sada set format\n",
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels_clarity\", \"labels_evasion\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels_clarity\", \"labels_evasion\", \"labels_evasion_annotators\"])"
      ],
      "metadata": {
        "id": "Jl_7EzTQDGXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBytJpJJHBE9",
        "outputId": "dfcf27b6-b82f-476e-f2d2-2e73ed51356a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([  101,  1053,  1012,  1997,  1996,  7226,  2368,  3447,  1012,  1998,\n",
            "         5496,  1996,  2142,  2163,  1997,  4820,  2859,  2096,  6183,  2005,\n",
            "         8041,  7566,  1012,  2129,  2052,  2017,  6869,  2000,  2008,  1029,\n",
            "         1998,  2079,  2017,  2228,  2343,  8418,  2003,  2108, 18006,  2055,\n",
            "         2893,  1996,  3276,  2067,  2006,  2650,  2004,  2002,  7221,  2015,\n",
            "         6207,  1999,  2859,  1029,  1064,  3437,  1024,  2092,  1010,  2298,\n",
            "         1010,  2034,  1997,  2035,  1010,  1996,  2072,  2572, 18006,  2055,\n",
            "         2893,  1996,  3276,  2157,  1012,  1998,  2028,  1997,  1996,  2477,\n",
            "         2008,  2003,  2183,  2006,  2085,  2003,  1010,  2859,  2003,  2927,\n",
            "         2000,  2689,  2070,  1997,  1996,  3513,  1997,  1996,  2208,  1010,\n",
            "         1999,  3408,  1997,  3119,  1998,  2060,  3314,  1012,  1998,  2061,\n",
            "         2028,  1997,  1996,  2477,  2057,  5720,  2055,  1010,  2005,  2742,\n",
            "         1010,  2003,  2008,  2027,  1005,  2128,  2085,  3331,  2055,  2437,\n",
            "         2469,  2008,  2053,  2822,  3630,  2028,  1999,  1996,  2822,  2231,\n",
            "         2064,  2224,  1037,  2530,  3526,  3042,  1012,  2216,  7957,  1997,\n",
            "         2477,  1012,  1998,  2061,  1010,  2428,  1010,  2054,  2023,  4440,\n",
            "         2001,  2055,  4183,  2001,  2625,  2055,  4820,  2859,  1012,  1045,\n",
            "         2123,  1005,  1056,  2215,  2000,  5383,  2859,  1012,  1045,  2074,\n",
            "         2215,  2000,  2191,  2469,  2008,  2057,  2031,  1037,  3276,  2007,\n",
            "         2859,  2008,  2003,  2006,  1996,  2039,  1998,  2039,  1010, 19942,\n",
            "         2185,  1010,  7955,  4282,  2054,  2009,  1005,  1055,  2035,  2055,\n",
            "         1012,  1998,  2028,  1997,  1996,  3971,  2017,  2079,  2008,  2003,\n",
            "         1010,  2017,  2191,  2469,  2008,  2057,  2024,  3331,  2055,  1996,\n",
            "         2168,  2477,  1012,  1998,  1045,  2228,  2008,  2028,  1997,  1996,\n",
            "         2477,  2057,  1005,  2310,  2589,  2072,  1005,  2310,  2699,  2000,\n",
            "         2079,  1010,  1998,  1045,  1005,   102]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'labels_clarity': tensor(0), 'labels_evasion': tensor(5)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multitask_collator(batch):\n",
        "    collated_batch = {\n",
        "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
        "        \"labels_clarity\": torch.stack([x[\"labels_clarity\"] for x in batch]),\n",
        "    }\n",
        "\n",
        "    # Add labels_evasion if present (for train_dataset and placeholder in test_dataset)\n",
        "    if \"labels_evasion\" in batch[0]:\n",
        "        collated_batch[\"labels_evasion\"] = torch.stack([x[\"labels_evasion\"] for x in batch])\n",
        "\n",
        "    # Add labels_evasion_annotators if present (only for test_dataset)\n",
        "    if \"labels_evasion_annotators\" in batch[0]:\n",
        "        collated_batch[\"labels_evasion_annotators\"] = torch.stack([x[\"labels_evasion_annotators\"] for x in batch])\n",
        "\n",
        "    return collated_batch"
      ],
      "metadata": {
        "id": "le9YyPSyk1-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    clarity_logits, evasion_logits = eval_pred.predictions\n",
        "\n",
        "    # Unpack label_ids: clarity labels, the -1 placeholder labels, and the annotator labels\n",
        "    labels_clarity_tensor, labels_evasion_placeholder_tensor, labels_evasion_annotators_tensor = eval_pred.label_ids\n",
        "\n",
        "    labels_clarity = labels_clarity_tensor.numpy()\n",
        "    labels_evasion_annotators = labels_evasion_annotators_tensor.numpy()\n",
        "\n",
        "    clarity_preds = np.argmax(clarity_logits, axis=1)\n",
        "    evasion_preds = np.argmax(evasion_logits, axis=1)\n",
        "\n",
        "    # --- Clarity Metrics ---\n",
        "    metrics = {\n",
        "        \"clarity_acc\": accuracy_score(labels_clarity, clarity_preds),\n",
        "        \"clarity_f1\": f1_score(labels_clarity, clarity_preds, average=\"macro\"),\n",
        "    }\n",
        "\n",
        "    # --- Evasion Metrics (using multi-annotator logic) ---\n",
        "    evasion_correct_predictions = 0\n",
        "    total_evasion_samples_with_annotators = 0\n",
        "\n",
        "    true_labels_for_f1_evasion = []\n",
        "    pred_labels_for_f1_evasion = []\n",
        "\n",
        "    for i in range(len(evasion_preds)):\n",
        "        pred = evasion_preds[i]\n",
        "        # Get valid annotator labels for this sample (excluding -1 placeholders)\n",
        "        valid_annotators_for_sample = labels_evasion_annotators[i][labels_evasion_annotators[i] != -1]\n",
        "\n",
        "        if len(valid_annotators_for_sample) > 0:\n",
        "            total_evasion_samples_with_annotators += 1\n",
        "            # Check if prediction matches ANY valid annotator\n",
        "            if pred in valid_annotators_for_sample:\n",
        "                evasion_correct_predictions += 1\n",
        "\n",
        "            # For F1, use the first valid annotator as the 'true' label for that sample.\n",
        "            # This is a simplification for calculating F1 in a multi-annotator scenario.\n",
        "            true_labels_for_f1_evasion.append(valid_annotators_for_sample[0])\n",
        "            pred_labels_for_f1_evasion.append(pred)\n",
        "\n",
        "    if total_evasion_samples_with_annotators > 0:\n",
        "        evasion_accuracy = evasion_correct_predictions / total_evasion_samples_with_annotators\n",
        "        evasion_f1 = f1_score(true_labels_for_f1_evasion, pred_labels_for_f1_evasion, average=\"macro\")\n",
        "        metrics.update({\n",
        "            \"evasion_acc\": evasion_accuracy,\n",
        "            \"evasion_f1\": evasion_f1\n",
        "        })\n",
        "    else:\n",
        "        metrics.update({\n",
        "            \"evasion_acc\": 0.0,\n",
        "            \"evasion_f1\": 0.0\n",
        "        })\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "4B0dpWU4k_6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"qevasion_multitask\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=multitask_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhf50O20k-cv",
        "outputId": "eb6a9cec-faee-4eb0-a439-580538c91570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Z-_q-jnIlDZj",
        "outputId": "58ecd7e1-7b62-4f4d-bf36-1608d08bae3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='648' max='648' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [648/648 04:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Clarity Acc</th>\n",
              "      <th>Clarity F1</th>\n",
              "      <th>Evasion Acc</th>\n",
              "      <th>Evasion F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.761000</td>\n",
              "      <td>0.826547</td>\n",
              "      <td>0.600649</td>\n",
              "      <td>0.523880</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.935900</td>\n",
              "      <td>0.826062</td>\n",
              "      <td>0.584416</td>\n",
              "      <td>0.523328</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.660000</td>\n",
              "      <td>0.813663</td>\n",
              "      <td>0.626623</td>\n",
              "      <td>0.537545</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=648, training_loss=1.8436029222276475, metrics={'train_runtime': 273.0206, 'train_samples_per_second': 37.887, 'train_steps_per_second': 2.373, 'total_flos': 0.0, 'train_loss': 1.8436029222276475, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}