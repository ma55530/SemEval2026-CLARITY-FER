{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ma55530/SemEval2026-CLARITY-FER/blob/main/hierarchiral/TwoLayeredBinary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pclIURvraWE"
      },
      "source": [
        "# **Installing the necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE-6wjEfrSAm"
      },
      "outputs": [],
      "source": [
        "pip -q install scikit-learn torch pandas datasets transformers torchvision accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJXVboHZrVPw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUVP4_AJsvgY"
      },
      "source": [
        "# **Loading the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRTDHxmFs6xM"
      },
      "outputs": [],
      "source": [
        "print(\"Loading dataset...\")\n",
        "df = load_dataset(\"ailsntua/QEvasion\")\n",
        "\n",
        "def clarity_to_label(row):\n",
        "    mapping = {\n",
        "        \"Clear Reply\": 0,\n",
        "        \"Ambivalent\": 1,\n",
        "        \"Clear Non-Reply\": 2\n",
        "    }\n",
        "    row[\"label\"] = mapping[row[\"clarity_label\"]]\n",
        "\n",
        "    # --- FIXED SHORT/LONG BINARY LABEL ---\n",
        "    # This line was incorrect: row[\"binary_label\"] = 0 if (row[\"clarity_label\"] == 0 or row[\"clarity_label\"] == 1) else 1\n",
        "    # It should compare with the assigned integer label:\n",
        "    row[\"binary_label\"] = 0 if (row[\"label\"] == 0 or row[\"label\"] == 1) else 1\n",
        "    return row\n",
        "\n",
        "df = df.map(clarity_to_label)\n",
        "y_test = df[\"test\"][\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN0D-pt9yQgU"
      },
      "source": [
        "# **Defining the evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11IjYlyFyw61"
      },
      "outputs": [],
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
        "    plt.show()\n",
        "\n",
        "def compute_metrics_binary(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"precision\": precision_score(labels, preds, average='binary', zero_division=0),\n",
        "        \"recall\": recall_score(labels, preds, average='binary', zero_division=0),\n",
        "        \"f1\": f1_score(labels, preds, average='binary', zero_division=0)\n",
        "    }\n",
        "\n",
        "def compute_metrics_fine(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"precision\": precision_score(labels, preds, average='macro', zero_division=0),\n",
        "        \"recall\": recall_score(labels, preds, average='macro', zero_division=0),\n",
        "        \"f1\": f1_score(labels, preds, average='macro', zero_division=0)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1hMCrvF3ZK8"
      },
      "source": [
        "# **Loading the model and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmP5s3bC3gq4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "binary_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=2\n",
        ")\n",
        "\n",
        "fine_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=2\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize(example):\n",
        "    encoded = tokenizer(\n",
        "        \"Question: \" + example[\"interview_question\"] +\n",
        "        \" Answer: \" + example[\"interview_answer\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "        truncation=True\n",
        "    )\n",
        "    encoded[\"label\"] = example[\"label\"]\n",
        "    encoded[\"binary_label\"] = example[\"binary_label\"]\n",
        "    return encoded\n",
        "\n",
        "tokenized_train = df[\"train\"].map(tokenize)\n",
        "tokenized_test = df[\"test\"].map(tokenize)\n",
        "\n",
        "cols_to_remove = [\"interview_question\", \"interview_answer\", \"clarity_label\"]\n",
        "\n",
        "tokenized_train = tokenized_train.remove_columns(cols_to_remove)\n",
        "tokenized_test  = tokenized_test.remove_columns(cols_to_remove)\n",
        "\n",
        "tokenized_train.set_format(\"torch\")\n",
        "tokenized_test.set_format(\"torch\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkrH4UON_dU0"
      },
      "source": [
        "# **Preparing the data for training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLqGCRXu_iNV"
      },
      "outputs": [],
      "source": [
        "binary_train = tokenized_train.remove_columns([\"label\"]).rename_column(\"binary_label\", \"labels\")\n",
        "binary_test  = tokenized_test.remove_columns([\"label\"]).rename_column(\"binary_label\", \"labels\")\n",
        "\n",
        "train_fine = tokenized_train.filter(lambda x: x[\"binary_label\"].item() == 0 and x[\"label\"].item() in [0,1]).remove_columns([\"binary_label\"])\n",
        "test_fine  = tokenized_test.filter(lambda x: x[\"binary_label\"].item() == 0 and x[\"label\"].item() in [0,1]).remove_columns([\"binary_label\"])\n",
        "\n",
        "train_fine = train_fine.rename_column(\"label\", \"labels\")\n",
        "test_fine  = test_fine.rename_column(\"label\", \"labels\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "labels_np = np.array(df[\"train\"][\"label\"])\n",
        "class_weights_np = compute_class_weight(\"balanced\", classes=np.unique(labels_np), y=labels_np)\n",
        "class_weights = torch.tensor(class_weights_np, dtype=torch.float)\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "print(set(train_fine[\"labels\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbbQ12O2qIA8"
      },
      "source": [
        "# **Defining the evaluation function and training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EP3XT3BoqONl"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "class WeightedTrainer(Trainer):\n",
        "    def __init__(self, weights=None, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.weights = weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs[\"labels\"]\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.weights.to(model.device))\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "\n",
        "trainer_binary = WeightedTrainer(\n",
        "    weights=torch.tensor([1.0, 1.0]),\n",
        "    model=binary_model,\n",
        "    train_dataset=binary_train,\n",
        "    eval_dataset=binary_test,\n",
        "    compute_metrics=compute_metrics_binary,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"binary_output\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        fp16=True,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        ")\n",
        "trainer_binary.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_fine = Trainer(\n",
        "    model=fine_model,\n",
        "    train_dataset=train_fine,\n",
        "    eval_dataset=test_fine,\n",
        "    compute_metrics=compute_metrics_fine,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"fine_output_unweighted\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        fp16=True,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_fine.train()\n"
      ],
      "metadata": {
        "id": "fTbP3HR8556P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxO4CzDCAdDE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def classify_two_stage(example):\n",
        "    with torch.no_grad():\n",
        "        logits = binary_model(**example).logits\n",
        "    binary_pred = logits.argmax(dim=-1).item()\n",
        "\n",
        "    if binary_pred == 1:\n",
        "        return 2  # Clear Non-Reply\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = fine_model(**example).logits\n",
        "    fine_pred = logits.argmax(dim=-1).item()\n",
        "    return fine_pred\n",
        "\n",
        "def plot_cm(trainer, dataset, display_labels):\n",
        "    predictions = trainer.predict(dataset)\n",
        "    preds = np.argmax(predictions.predictions, axis=-1)\n",
        "    labels = predictions.label_ids\n",
        "    ConfusionMatrixDisplay.from_predictions(labels, preds, display_labels=display_labels)\n",
        "    plt.show()\n",
        "\n",
        "plot_cm(trainer_binary, binary_test, [\"Clear Reply / Ambivalent\", \"Clear Non-Reply\"])\n",
        "plot_cm(trainer_fine, test_fine, [\"Clear Reply\", \"Ambivalent\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}